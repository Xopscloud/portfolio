---
title: "AWS Kinesis: Building Real-Time Data Streaming Pipelines"
publishedAt: "2024-09-14"
summary: "Complete guide to implementing AWS Kinesis for real-time data processing, from basic streams to advanced analytics with practical examples."
image: "/images/blog/aws-kinesis.jpg"
---

Real-time data processing has become essential for modern applications. AWS Kinesis provides a powerful platform for ingesting, processing, and analyzing streaming data at scale. Here's everything you need to know about building production-ready streaming pipelines with Kinesis.

## Understanding AWS Kinesis Services

### Kinesis Data Streams
Real-time data ingestion and processing with custom applications.

### Kinesis Data Firehose
Fully managed service for delivering streaming data to destinations like S3, Redshift, and Elasticsearch.

### Kinesis Data Analytics
Real-time analytics on streaming data using SQL or Apache Flink.

### Kinesis Video Streams
Securely stream video from connected devices for analytics and machine learning.

## Setting Up Kinesis Data Streams

### Basic Stream Configuration
```yaml
# CloudFormation template for Kinesis Stream
Resources:
  KinesisStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: user-activity-stream
      ShardCount: 2
      RetentionPeriodHours: 24
      ShardLevelMetrics:
        - IncomingRecords
        - OutgoingRecords
      StreamEncryption:
        EncryptionType: KMS
        KeyId: alias/aws/kinesis
      Tags:
        - Key: Environment
          Value: production
        - Key: Application
          Value: analytics
```

### Producer Implementation
```python
import boto3
import json
import time
from datetime import datetime
import uuid

class KinesisProducer:
    def __init__(self, stream_name, region='us-east-1'):
        self.kinesis_client = boto3.client('kinesis', region_name=region)
        self.stream_name = stream_name
    
    def put_record(self, data, partition_key=None):
        """Put a single record to Kinesis stream"""
        if partition_key is None:
            partition_key = str(uuid.uuid4())
        
        try:
            response = self.kinesis_client.put_record(
                StreamName=self.stream_name,
                Data=json.dumps(data),
                PartitionKey=partition_key
            )
            return response
        except Exception as e:
            print(f"Error putting record: {e}")
            return None
    
    def put_records_batch(self, records):
        """Put multiple records in batch for better throughput"""
        kinesis_records = []
        
        for record in records:
            kinesis_records.append({
                'Data': json.dumps(record['data']),
                'PartitionKey': record.get('partition_key', str(uuid.uuid4()))
            })
        
        try:
            response = self.kinesis_client.put_records(
                Records=kinesis_records,
                StreamName=self.stream_name
            )
            
            # Handle failed records
            failed_records = []
            for i, record in enumerate(response['Records']):
                if 'ErrorCode' in record:
                    failed_records.append(records[i])
            
            return {
                'successful_count': len(records) - len(failed_records),
                'failed_records': failed_records
            }
        except Exception as e:
            print(f"Error putting batch records: {e}")
            return None

# Usage example
producer = KinesisProducer('user-activity-stream')

# Single record
user_event = {
    'user_id': '12345',
    'event_type': 'page_view',
    'page': '/products',
    'timestamp': datetime.utcnow().isoformat(),
    'session_id': 'sess_67890'
}

producer.put_record(user_event, partition_key=user_event['user_id'])

# Batch records
batch_events = [
    {
        'data': {
            'user_id': '12346',
            'event_type': 'purchase',
            'product_id': 'prod_123',
            'amount': 99.99,
            'timestamp': datetime.utcnow().isoformat()
        },
        'partition_key': '12346'
    },
    {
        'data': {
            'user_id': '12347',
            'event_type': 'cart_add',
            'product_id': 'prod_456',
            'timestamp': datetime.utcnow().isoformat()
        },
        'partition_key': '12347'
    }
]

result = producer.put_records_batch(batch_events)
print(f"Successfully sent {result['successful_count']} records")
```

### Consumer Implementation
```python
import boto3
import json
import time
from datetime import datetime

class KinesisConsumer:
    def __init__(self, stream_name, region='us-east-1'):
        self.kinesis_client = boto3.client('kinesis', region_name=region)
        self.stream_name = stream_name
        self.shard_iterators = {}
    
    def get_shard_iterator(self, shard_id, iterator_type='LATEST'):
        """Get shard iterator for reading records"""
        try:
            response = self.kinesis_client.get_shard_iterator(
                StreamName=self.stream_name,
                ShardId=shard_id,
                ShardIteratorType=iterator_type
            )
            return response['ShardIterator']
        except Exception as e:
            print(f"Error getting shard iterator: {e}")
            return None
    
    def process_records(self, records):
        """Process the received records"""
        for record in records:
            try:
                # Decode the data
                data = json.loads(record['Data'])
                partition_key = record['PartitionKey']
                sequence_number = record['SequenceNumber']
                
                print(f"Processing record: {data}")
                
                # Add your business logic here
                self.handle_event(data)
                
            except Exception as e:
                print(f"Error processing record: {e}")
    
    def handle_event(self, event_data):
        """Handle individual event based on event type"""
        event_type = event_data.get('event_type')
        
        if event_type == 'page_view':
            self.handle_page_view(event_data)
        elif event_type == 'purchase':
            self.handle_purchase(event_data)
        elif event_type == 'cart_add':
            self.handle_cart_add(event_data)
        else:
            print(f"Unknown event type: {event_type}")
    
    def handle_page_view(self, data):
        """Handle page view events"""
        print(f"User {data['user_id']} viewed page {data['page']}")
        # Update analytics, send to database, etc.
    
    def handle_purchase(self, data):
        """Handle purchase events"""
        print(f"User {data['user_id']} purchased product {data['product_id']} for ${data['amount']}")
        # Update inventory, send confirmation email, etc.
    
    def handle_cart_add(self, data):
        """Handle cart add events"""
        print(f"User {data['user_id']} added product {data['product_id']} to cart")
        # Update recommendations, trigger remarketing, etc.
    
    def consume_stream(self):
        """Main consumer loop"""
        # Get stream description to find shards
        stream_description = self.kinesis_client.describe_stream(
            StreamName=self.stream_name
        )
        
        shards = stream_description['StreamDescription']['Shards']
        
        # Initialize shard iterators
        for shard in shards:
            shard_id = shard['ShardId']
            iterator = self.get_shard_iterator(shard_id)
            if iterator:
                self.shard_iterators[shard_id] = iterator
        
        # Consume records
        while True:
            for shard_id, iterator in list(self.shard_iterators.items()):
                try:
                    response = self.kinesis_client.get_records(
                        ShardIterator=iterator,
                        Limit=100
                    )
                    
                    records = response['Records']
                    if records:
                        self.process_records(records)
                    
                    # Update iterator for next batch
                    next_iterator = response.get('NextShardIterator')
                    if next_iterator:
                        self.shard_iterators[shard_id] = next_iterator
                    else:
                        # Shard is closed
                        del self.shard_iterators[shard_id]
                
                except Exception as e:
                    print(f"Error reading from shard {shard_id}: {e}")
                    time.sleep(1)
            
            time.sleep(1)  # Avoid hitting API limits

# Usage
consumer = KinesisConsumer('user-activity-stream')
consumer.consume_stream()
```

## Kinesis Data Firehose Implementation

### Firehose Delivery Stream
```yaml
Resources:
  DeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: analytics-delivery-stream
      DeliveryStreamType: DirectPut
      S3DestinationConfiguration:
        BucketARN: !GetAtt S3Bucket.Arn
        Prefix: "year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/"
        ErrorOutputPrefix: "errors/"
        BufferingHints:
          SizeInMBs: 5
          IntervalInSeconds: 300
        CompressionFormat: GZIP
        RoleARN: !GetAtt FirehoseRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt DataTransformLambda.Arn
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref FirehoseLogGroup

  DataTransformLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: kinesis-data-transform
      Runtime: python3.9
      Handler: index.lambda_handler
      Code:
        ZipFile: |
          import json
          import base64
          from datetime import datetime
          
          def lambda_handler(event, context):
              output = []
              
              for record in event['records']:
                  # Decode the data
                  payload = base64.b64decode(record['data'])
                  data = json.loads(payload)
                  
                  # Transform the data
                  transformed_data = {
                      'user_id': data.get('user_id'),
                      'event_type': data.get('event_type'),
                      'timestamp': data.get('timestamp'),
                      'processed_at': datetime.utcnow().isoformat(),
                      'source': 'kinesis-firehose'
                  }
                  
                  # Add additional fields based on event type
                  if data.get('event_type') == 'purchase':
                      transformed_data['revenue'] = data.get('amount', 0)
                      transformed_data['product_id'] = data.get('product_id')
                  
                  # Encode the transformed data
                  output_record = {
                      'recordId': record['recordId'],
                      'result': 'Ok',
                      'data': base64.b64encode(
                          (json.dumps(transformed_data) + '\n').encode('utf-8')
                      ).decode('utf-8')
                  }
                  
                  output.append(output_record)
              
              return {'records': output}
```

### Firehose Producer
```python
import boto3
import json
from datetime import datetime

class FirehoseProducer:
    def __init__(self, delivery_stream_name, region='us-east-1'):
        self.firehose_client = boto3.client('firehose', region_name=region)
        self.delivery_stream_name = delivery_stream_name
    
    def put_record(self, data):
        """Put a single record to Firehose"""
        try:
            response = self.firehose_client.put_record(
                DeliveryStreamName=self.delivery_stream_name,
                Record={
                    'Data': json.dumps(data) + '\n'  # Add newline for proper formatting
                }
            )
            return response
        except Exception as e:
            print(f"Error putting record to Firehose: {e}")
            return None
    
    def put_record_batch(self, records):
        """Put multiple records in batch"""
        firehose_records = []
        
        for record in records:
            firehose_records.append({
                'Data': json.dumps(record) + '\n'
            })
        
        try:
            response = self.firehose_client.put_record_batch(
                DeliveryStreamName=self.delivery_stream_name,
                Records=firehose_records
            )
            
            return {
                'successful_count': response['RequestResponses'].count({'RecordId': True}),
                'failed_count': response['FailedPutCount']
            }
        except Exception as e:
            print(f"Error putting batch to Firehose: {e}")
            return None

# Usage
firehose_producer = FirehoseProducer('analytics-delivery-stream')

# Send analytics events
events = [
    {
        'user_id': '12345',
        'event_type': 'page_view',
        'page': '/products',
        'timestamp': datetime.utcnow().isoformat(),
        'user_agent': 'Mozilla/5.0...',
        'ip_address': '192.168.1.1'
    },
    {
        'user_id': '12346',
        'event_type': 'purchase',
        'product_id': 'prod_123',
        'amount': 99.99,
        'timestamp': datetime.utcnow().isoformat()
    }
]

result = firehose_producer.put_record_batch(events)
print(f"Sent {result['successful_count']} records to S3")
```

## Kinesis Data Analytics

### SQL-based Analytics
```sql
-- Create in-application stream for real-time analytics
CREATE OR REPLACE STREAM "DESTINATION_SQL_STREAM" (
    user_id VARCHAR(32),
    event_type VARCHAR(32),
    event_count INTEGER,
    window_start TIMESTAMP,
    window_end TIMESTAMP
);

-- Real-time event counting with tumbling window
CREATE OR REPLACE PUMP "STREAM_PUMP" AS INSERT INTO "DESTINATION_SQL_STREAM"
SELECT STREAM 
    user_id,
    event_type,
    COUNT(*) as event_count,
    ROWTIME_TO_TIMESTAMP(RANGE_START) as window_start,
    ROWTIME_TO_TIMESTAMP(RANGE_END) as window_end
FROM SOURCE_SQL_STREAM_001
GROUP BY 
    user_id,
    event_type,
    RANGE(INTERVAL '1' MINUTE);

-- Anomaly detection for high-frequency events
CREATE OR REPLACE STREAM "ANOMALY_STREAM" (
    user_id VARCHAR(32),
    event_count INTEGER,
    avg_count DOUBLE,
    is_anomaly BOOLEAN,
    window_start TIMESTAMP
);

CREATE OR REPLACE PUMP "ANOMALY_PUMP" AS INSERT INTO "ANOMALY_STREAM"
SELECT STREAM
    user_id,
    event_count,
    avg_count,
    CASE 
        WHEN event_count > (avg_count * 3) THEN true 
        ELSE false 
    END as is_anomaly,
    window_start
FROM (
    SELECT STREAM
        user_id,
        COUNT(*) as event_count,
        AVG(COUNT(*)) OVER (
            PARTITION BY user_id 
            RANGE INTERVAL '10' MINUTE PRECEDING
        ) as avg_count,
        ROWTIME_TO_TIMESTAMP(RANGE_START) as window_start
    FROM SOURCE_SQL_STREAM_001
    GROUP BY 
        user_id,
        RANGE(INTERVAL '1' MINUTE)
);
```

### Analytics Application Configuration
```yaml
Resources:
  KinesisAnalyticsApplication:
    Type: AWS::KinesisAnalytics::Application
    Properties:
      ApplicationName: real-time-analytics
      ApplicationDescription: "Real-time user behavior analytics"
      ApplicationCode: !Sub |
        -- SQL code for real-time analytics
        CREATE OR REPLACE STREAM "DESTINATION_SQL_STREAM" (
            user_id VARCHAR(32),
            event_type VARCHAR(32),
            event_count INTEGER,
            revenue DECIMAL(10,2),
            window_start TIMESTAMP
        );
        
        CREATE OR REPLACE PUMP "STREAM_PUMP" AS INSERT INTO "DESTINATION_SQL_STREAM"
        SELECT STREAM 
            user_id,
            event_type,
            COUNT(*) as event_count,
            SUM(CASE WHEN event_type = 'purchase' THEN amount ELSE 0 END) as revenue,
            ROWTIME_TO_TIMESTAMP(RANGE_START) as window_start
        FROM SOURCE_SQL_STREAM_001
        GROUP BY 
            user_id,
            event_type,
            RANGE(INTERVAL '5' MINUTE);
      
      Inputs:
        - NamePrefix: "SOURCE_SQL_STREAM"
          KinesisStreamsInput:
            ResourceARN: !GetAtt KinesisStream.Arn
            RoleARN: !GetAtt AnalyticsRole.Arn
          InputSchema:
            RecordColumns:
              - Name: "user_id"
                SqlType: "VARCHAR(32)"
                Mapping: "$.user_id"
              - Name: "event_type"
                SqlType: "VARCHAR(32)"
                Mapping: "$.event_type"
              - Name: "amount"
                SqlType: "DECIMAL(10,2)"
                Mapping: "$.amount"
              - Name: "timestamp"
                SqlType: "TIMESTAMP"
                Mapping: "$.timestamp"
            RecordFormat:
              RecordFormatType: "JSON"
              MappingParameters:
                JSONMappingParameters:
                  RecordRowPath: "$"
```

## Monitoring and Scaling

### CloudWatch Metrics
```python
import boto3

def monitor_kinesis_metrics(stream_name):
    cloudwatch = boto3.client('cloudwatch')
    
    # Get key metrics
    metrics = [
        'IncomingRecords',
        'OutgoingRecords',
        'WriteProvisionedThroughputExceeded',
        'ReadProvisionedThroughputExceeded',
        'IteratorAgeMilliseconds'
    ]
    
    for metric in metrics:
        response = cloudwatch.get_metric_statistics(
            Namespace='AWS/Kinesis',
            MetricName=metric,
            Dimensions=[
                {
                    'Name': 'StreamName',
                    'Value': stream_name
                }
            ],
            StartTime=datetime.utcnow() - timedelta(hours=1),
            EndTime=datetime.utcnow(),
            Period=300,
            Statistics=['Sum', 'Average', 'Maximum']
        )
        
        print(f"{metric}: {response['Datapoints']}")

# Auto-scaling based on metrics
def auto_scale_kinesis_stream(stream_name, target_utilization=70):
    kinesis = boto3.client('kinesis')
    cloudwatch = boto3.client('cloudwatch')
    
    # Get current shard count
    stream_desc = kinesis.describe_stream(StreamName=stream_name)
    current_shards = len(stream_desc['StreamDescription']['Shards'])
    
    # Get utilization metrics
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/Kinesis',
        MetricName='IncomingRecords',
        Dimensions=[{'Name': 'StreamName', 'Value': stream_name}],
        StartTime=datetime.utcnow() - timedelta(minutes=15),
        EndTime=datetime.utcnow(),
        Period=300,
        Statistics=['Average']
    )
    
    if response['Datapoints']:
        avg_records = sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])
        records_per_shard = avg_records / current_shards
        
        # Scale up if utilization is high
        if records_per_shard > (1000 * target_utilization / 100):  # 1000 records/sec per shard capacity
            new_shard_count = current_shards + 1
            kinesis.update_shard_count(
                StreamName=stream_name,
                TargetShardCount=new_shard_count,
                ScalingType='UNIFORM_SCALING'
            )
            print(f"Scaled up to {new_shard_count} shards")
```

## Best Practices and Optimization

### Partition Key Strategy
```python
import hashlib

def get_optimal_partition_key(user_id, event_type):
    """
    Create partition key that ensures even distribution
    """
    # Combine user_id and event_type for better distribution
    combined = f"{user_id}:{event_type}"
    
    # Use hash to ensure even distribution across shards
    hash_value = hashlib.md5(combined.encode()).hexdigest()
    
    return hash_value[:8]  # Use first 8 characters

def get_time_based_partition_key():
    """
    Time-based partitioning for chronological processing
    """
    import time
    
    # Use minute-level precision for time-based partitioning
    timestamp = int(time.time() / 60)  # Minutes since epoch
    return str(timestamp)
```

### Error Handling and Retry Logic
```python
import time
import random
from botocore.exceptions import ClientError

class KinesisProducerWithRetry:
    def __init__(self, stream_name, max_retries=3):
        self.kinesis_client = boto3.client('kinesis')
        self.stream_name = stream_name
        self.max_retries = max_retries
    
    def put_record_with_retry(self, data, partition_key):
        """Put record with exponential backoff retry"""
        for attempt in range(self.max_retries + 1):
            try:
                response = self.kinesis_client.put_record(
                    StreamName=self.stream_name,
                    Data=json.dumps(data),
                    PartitionKey=partition_key
                )
                return response
            
            except ClientError as e:
                error_code = e.response['Error']['Code']
                
                if error_code == 'ProvisionedThroughputExceededException':
                    if attempt < self.max_retries:
                        # Exponential backoff with jitter
                        wait_time = (2 ** attempt) + random.uniform(0, 1)
                        time.sleep(wait_time)
                        continue
                    else:
                        raise e
                else:
                    # Non-retryable error
                    raise e
            
            except Exception as e:
                print(f"Unexpected error: {e}")
                raise e
```

## Cost Optimization

### Shard Management
```python
def optimize_shard_count(stream_name):
    """
    Analyze stream usage and recommend optimal shard count
    """
    kinesis = boto3.client('kinesis')
    cloudwatch = boto3.client('cloudwatch')
    
    # Get stream metrics for the last 24 hours
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=24)
    
    metrics = cloudwatch.get_metric_statistics(
        Namespace='AWS/Kinesis',
        MetricName='IncomingRecords',
        Dimensions=[{'Name': 'StreamName', 'Value': stream_name}],
        StartTime=start_time,
        EndTime=end_time,
        Period=3600,  # 1 hour periods
        Statistics=['Average', 'Maximum']
    )
    
    if not metrics['Datapoints']:
        return "No data available"
    
    # Calculate peak and average throughput
    peak_records = max(dp['Maximum'] for dp in metrics['Datapoints'])
    avg_records = sum(dp['Average'] for dp in metrics['Datapoints']) / len(metrics['Datapoints'])
    
    # Kinesis shard capacity: 1000 records/sec or 1MB/sec
    recommended_shards_peak = max(1, int(peak_records / 1000) + 1)
    recommended_shards_avg = max(1, int(avg_records / 1000) + 1)
    
    current_shards = len(kinesis.describe_stream(StreamName=stream_name)['StreamDescription']['Shards'])
    
    return {
        'current_shards': current_shards,
        'recommended_for_peak': recommended_shards_peak,
        'recommended_for_average': recommended_shards_avg,
        'peak_throughput': peak_records,
        'average_throughput': avg_records
    }
```

## Key Takeaways

1. **Choose the Right Service**: Use Data Streams for custom processing, Firehose for simple delivery, Analytics for real-time insights
2. **Partition Strategy**: Design partition keys for even distribution and parallel processing
3. **Monitor Continuously**: Track throughput, latency, and error rates
4. **Handle Failures**: Implement retry logic and error handling
5. **Optimize Costs**: Right-size shards based on actual usage patterns
6. **Security First**: Enable encryption and proper IAM policies

AWS Kinesis provides powerful capabilities for real-time data processing, but success depends on proper architecture, monitoring, and optimization. Start with simple use cases and gradually build more complex streaming analytics as your requirements grow.

What real-time data challenges are you solving with Kinesis? Share your streaming architecture patterns in the comments!